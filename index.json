[{"categories":null,"content":"RO-Crate is a method for describing and packaging research data from ANY discipline into distributable, reusable Digital Objects with any amount of detailed metadata from simple who/what/where discovery-oriented description to metadata at the file-level or even variable level inside files. RO-Crate is an implementer-focussed guide to best practice and is based on widely-used standards with schema.org annotations in JSON-LD and aims to make it easy to creat good quality metadata description tools which are accessible and practical for use in a wide variety of situations; from an individual researcher working with a folder of data, to large data-intensive computational research environments. RO-Crate is used in the UTS Research Data Portal and the Modern PARADISEC demonstrator for all data objects. RO-Crate is method for describing a dataset as a digital object using a single linked-data metadata document Each resource can have a machine readable description in JSON-LD format A human-readable description and preview can be in an HTML file that lives alongside the metadata Provenance and workflow information can be included - to assist in data and research-process re-use RO-Crate Digital Objects may be packaged for distribution eg via Zip, Bagit and OCFL Objects ","date":"2024-01-01","objectID":"/standards/ro-crate/:0:0","tags":null,"title":"Metadata: Research Object Crate (RO-Crate)","uri":"/standards/ro-crate/"},{"categories":null,"content":"Oxford Common File Layout (OCFL) is a specification for laying out digital collections on file or object storage. It is designed with long-term preservation principles in mind; does not rely on specialised software and avoids the problem of locking data collections into monolithic repositories behind APIs. An OCFL Repository is a system of directories laid out on a filesystem using a prescribed layout. Each repository contains one or more OCFL Objects. An object contains an inventory and a set of versioned content directories. The metadata describing an OCFL object’s inventory and versions is stored as simple JSON files which are both human- and machine-readable, and can be processed with lightweight scripts. The structure inside the content directories is not specified, so any existing collection of files can be deposited into an OCFL repository and later re-exported with its structure preserved. OCFL places no restrictions on the file formats of object contents. ","date":"2024-01-01","objectID":"/standards/ocfl/:0:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"OCFL provides Robustness against file errors and data corruption Efficient versioning and de-duplication Immutable data storage ","date":"2024-01-01","objectID":"/standards/ocfl/:1:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"Links OCFL website OCFL Specification v1.0 ","date":"2024-01-01","objectID":"/standards/ocfl/:2:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"Example Here is a simplified view of an OCFL repository containing two objects, one of which has two versions and one with three: - OCFL repository - Object A - inventory - v1 - inventory - content - v2 - inventory - content - Object B - v1 - inventory - content - v2 - inventory - content - v3 - inventory - content ","date":"2024-01-01","objectID":"/standards/ocfl/:3:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"RRKive is an evolution of of the Arkisto website. This was created at the University of Technology Sydney with a small group of partners, and served to bring together a community of like-minded. The tools mentioned on the Arkisto site were mentioned in the Language Data Commons of Australia proposal and the principles and ideas are still relevant, but we decided to make a new site which which better articulates the principles and is more flexible as to how they may be implemented. While LDaCA has successfully implemented a range of tools that implement the named Arkisto standards, full adoption of the initial standards stack did not suit all partners and the principles were not fully articulated, so with RRKive we aim to clearly separate principles \u0026 requirements from choice of standards, and from implementation in code. ","date":"1970-01-01","objectID":"/background/:0:0","tags":null,"title":"Background","uri":"/background/"},{"categories":null,"content":"Contact Us ","date":"1970-01-01","objectID":"/contact/:0:0","tags":null,"title":"Contact Us","uri":"/contact/"},{"categories":null,"content":"THIS IS A DRAFT / Work in progress that LDaCA staff are preparing for a workshop in early February 2024 – if you have suggestions, or issues - please open issues on our github project The RRKive.org website is an initiative of the Language Data Commons of Australia; one of the partners of the Australian Research Data Commons (ARDC). The initial version of the site (Q1 2024) is intended to be the start of a conversation with our partners and stakeholders, inviting them to critique and refine these principles with a view to collective adoption as part of manifesto, and/or standard architecture for Data Commons. Initially this is technical and word-heavy, but we aim to introduce some graphics to help explain the concepts – once we have brought on board more collaborators and refined these principles. What’s a Data Commons and where’s the data in one? Following the lead of Jenny Fewster, HASS and Indigenous Research Data Commons Director at the Australian Research Data Commons (ARDC) we use this definition for Data Commons (@grossmanCaseDataCommons2016): A global trusted system of systems that provides frictionless access to high quality interoperable resources, services and artefacts for research. The abstract of this article says: Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. NOTE: This definition differs from others in that it does not use the word “open”, which is important, as not all research data can be made openly available; according to the [FAIR] and [CARE] principles can and should be made accessible to the right people / agents. For some background on FAIR and CARE see this LDaCA blog post. This web site looks at the core services and infrastructure needed to undertake the processes mentioned in the definition: managing, analyzing, and sharing; we do that below, paying particular attention to where the data resides in a Data Commons. This is about Research Data Commons deployments This site is about: A set of principles and an architectural vision for sustainable Data Commons deployments, particularly for data management. A toolkit for deploying granular sustainable archival repository software which can describe and make data accessible down to and inside of the files and datasets but using commodity IT systems to ensure data interoperability Enabling the ‘interoperability’ mentioned in the definition of a Data Commons above. One of the key inspirations for RRKive was the approach taken by PARADISEC (@barwickUnlockingArchives2018) where data is stored and managed using a very simple architecture with data and metadata at its heart sitting, on top of commodity IT services; initially this was a file-system and is now cloud-based object storage. The key idea was that the data should always be available to administrators independently of particular software services (though because some data is not open it is not possible to just put data up on an open webserver – mediation is needed). This site is based on a previous effort know as Arkisto, see the background page. ","date":"1970-01-01","objectID":"/_index_source/:0:0","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Scope: what people, domains, institutions, kinds and scales of data is this relevant for? This site is for leaders and implementers of Research Data Commons projects AND general research data management practitioners looking to choose, manage or establish sustainable CARE and FAIR compliant data management solutions that will work with research services. This site is relevant to any research or cultural domain, where teams are establishing data management infrastructure – the initial uses case and implementations are mainly from the Humanities and Social Sciences, with some “sciencey” and generalist deployments. ","date":"1970-01-01","objectID":"/_index_source/:1:0","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Not in scope There are a number of data management / Data Commons scenarios where this will be less relevant. Parts of this site are less relevant for domains where very bit of existing and prospective data, and all analytical products and annotations have an identified home with: Persistent Identification Sustainable secure storage for raw and derived data and research outputs at useful granularity – eg down to individually addressable items, files or variables within files Appropriate access control Catalogues / portals to make data discoverable APIs to integrate and interoperate with analytical and data curation processes ","date":"1970-01-01","objectID":"/_index_source/:2:0","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Core principles for a sustainable FAIR/CARE data commons architecture: In research contexts, it has been common for investment to be prioritized in research tools for analysis and/or presentation, often at the cost of locking up data in software stacks that make re-use and long-term access difficult, or focus on short term projects with data without ensuring its longevity. Research teams and IT professionals often focus on product – doing novel analyses using data flows and integration without putting in place the services needed for research integrity; assigning IDs and ensuring that those IDs resolve to data over the long term. ","date":"1970-01-01","objectID":"/_index_source/:3:0","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Principle 1: Separate Archival Storage FUNCTIONS from Workspaces An overriding principle for the RRKive approach is to Separate concerns between: Workspaces where data is collected, curated, described and analyzed Archival repositories that provide data persistence, persistent ID resolution and appropriate access control ","date":"1970-01-01","objectID":"/_index_source/:3:1","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Principle 2: Ensure Archival Repository storage is not locked to a single software stack Keep data in a commodity IT storage system with more than one mode of access Divide up data into chunks (“objects”, “items”) Keep metadata in a standard format adjacent to data files Aim for “rebuildability” of services (eg catalogues \u0026 access control) from the storage system Include a natural-language license with each object setting out how data may be used and/or redistributed and by WHOM ","date":"1970-01-01","objectID":"/_index_source/:3:2","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"Principle 3: Use an extensible linked-data metadata format Linked Data allows: Any conceivable data structure to be described Vocabularies to be mixed-in as needed; from a core set for all data to domain-specific to project or even dataset-specific terms; this can be formalised using Profiles. Interoperability with global research information systems architectures, discovery services etc While LDaCA is part of the Humanities and Social Sciences and Indigenous Research Data Commons, this vision is by no means limited to that scope – these ideas are relevant to all domains where data is available as file-based objects at a scale that can be managed in file system-like storage.code ","date":"1970-01-01","objectID":"/_index_source/:3:3","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"References ","date":"1970-01-01","objectID":"/_index_source/:4:0","tags":null,"title":"RRKive Home","uri":"/_index_source/"},{"categories":null,"content":"(This page is a work in progress, first version is a quick update from the Arkisto website: will be updated with fresh examples in Q2 2024) There are multiple use cases for RRKive which we will document in the abstract, in addition to the specific case studies we’re already working on. Due to its standards-based and extensible nature, RRKive can realise the goal of making data FAIR (Findable, Accessible, Interoperable, Re-usable). The (mythical) minimal RRKive platform The simplest possible RRKive platform deployment would be a repository with some objects in it. No portal, no data ingest or preservation service, eg: That might not seem very useful in itself but a standards-based repository is a great candidate for data preservation - it puts the I in FAIR (Interoperable) data. It can also provide a basis for re-activation via services that can Reuse the data by making Findable and Accessible. Because of RRKive’s use of Standards, the repository is the core to which services can be added. Adding a web portal To make the data Findable - the F in FAIR Data a portal may be added - this requires some configuration but significantly less than building a service from scratch. For example the Oni toolkit is used by the Language Data Commons of Australia for its portals, the main one being https://data.ldaca.edu.au/search The same portal software has been used in other contexts and is being adapted at the University of Sydney for a variety of data collections. Web portals depend on the repository content being indexed - web pages are generated mostly from calls to the index, and thus can be highly performant. There is more on indexing below. Data ingest pathways But how does data get into an OCFL repository? There are several patterns in production, in development and planned. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:0:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"A snapshot repository Exporting a repository to RO-Crate can be a matter of writing a script to interrogate a database, convert static files, or otherwise traverse an existing dataset. This pattern was used by the 2020 snapshot of ExpertNation - where we were given an XML file exported from the Heurist Content Management System and used a script to convert that data to the RO-Crate format. This RO-Crate can in turn be deposited in a repository - in this case the UTS data repository - and served via a portal, preserving the data while at the same time making it Accessible. This pattern has also been used extensively in the Language Data Commons of Australia where individual collections of data are converted using a series of scripts - instead of creating an entire site from one collection as in Expert Nation, each collection is loaded into the site. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:1:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Field data capture Data from sensors in the field is often streamed directly to some kind of database with or without portal services and interfaces. There are multiple possible RRKive deployment patterns in this situation. Where practical, the UTS eResearch team aimed to take an approach that first keeps copies of any raw data files and preserves those. The team then built databases and discovery portals from the raw data, although this is not always possible. This diagram shows an approximation of one current scenario which was implemented at UTS where raw files were NOT available, but readings could be extracted from a Database: ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:2:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Analytical and housekeeping tools So far on this page we have covered simplified views of RRKive deployment patterns with the repository at the centre, adding discovery portals and giving examples of data-acquisition pipelines (just scratching the surface of the possibilities). These things in themselves have value: making sure data is well described and as future proof as possible are vitally important but what can we DO with data? ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"OCFL + RO-Crate tools Having data in a standard-stack, with OCFL used to lay-out research data objects and RO-Crate to describe them, means that it is possible to write simple programs that can interrogate a repository. That is, you don’t have to spend time understanding the organisation of each dataset. The same idea underpins RRKive’s web-portal tools: standardization reduces the cost of building. Validators and preservation tools: there are not many of these around yet, but members of the RRKive community as well as the broader OCFL and RO-Crate communities are working on these; expect to see preservation tools that work over OCFL repositories in particular. Brute-force scripts: for moderate-sized repositories, it is possible to write a scripts to examine every object in a repository and to run analytics. For instance, it would be possible to visualise a number of years’ worth of sensor readings from a small number of sensors or to look for the geographical distribution of events in historical datasets. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:1","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Adding databases and other indexes For larger-scale use, visiting every object in a repository can be inefficient. In these cases, using an index means that an analysis script can request all the data in a particular time-window or of a certain type - or any other query that the index supports. While the index engines used in our current portals are based on full-text search and metadata, we expect others to be built as needed by disciplines using, for example, SQL databases or triple stores. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:2","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Analysis tool integration Above, we have looked at how people or machines can access an RRKive platform deployment by querying the repository, either directly or via an index. However, there is a much larger opportunity in being able to integrate RRKive deployments with other tools in the research landscape. To take one example, text analysis is in great demand across a very wide range of disciplines. This hypothetical scenario shows the potential for a researcher to use a web portal to locate datasets which contain text and then send the whole results set to a an analysis platform, in this case an interactive Jupyter notebook. RRKive already allows re-use of visualisation tools and viewers that can be embedded directly in a portal. We are planning a “widget store” that will enable researchers to choose and deploy a number of add-ons to the basic portal. Institutional and discipline repositories One of the major use case deployment patterns for RRKive is to underpin an institutional data repository / archive function, see the UTS data repository for an established example. In this use case, data is ingested into the repository via a research data management system which talks to the OCFL repository, not the portal. There is a clear separation of concerns: the portal’s job is to provide controlled access and search services via an index, the OCFL repository keeps version controlled data on disc, and the Research Data Management System handles deposit of data. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:3","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Manual archiving At UTS the Research Data Management system in use is RedBox - an open source platform for managing data across the research process from research data management planning (via Research Data Management Plans (RDMPS)) to archiving and re-use of data. ReDBox has services for provisioning and/or tracking Research Workspaces, which are sites where research data collection and management. All of the data acquisition scenarios described above would qualify as Research Workspaces, as do file-shares on institutional storage or share-sync services such as CloudStor, as well as survey platforms, project management and version control systems such as Gitlab and Github. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:4:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Publishing data The UTS institutional data repository actually has two parts: an internal behind-the-firewall archive, with access control - to ensure that only authorized people can access data - and an external data portal for publicly accessible data. This architecture reduces the risk of data breaches by not allowing access through the firewall to sensitive or confidential data until secure tools are available to allow extra-institutional access. Researchers can select a subset of an archived dataset to be published, or publish an entire dataset. A “bot” “notices” that a new public dataset is available and copies it to the public repository, where it will be indexed and made available through the data portal. NOTE: There is no monolithic “Repository Application” that mediates all interactions with the file-base OCFL store but a set of services which operate independently. This does mean that processes must be in place to ensure that there is not file-contention, with two bits of software trying to update an object at the same time. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:5:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"FAIR \u0026 CARE The FAIR and/or CARE principles are frequently mandated in various contexts around the world, inclduing for outputs of the Australian Research Data Commons (ARDC). These principles are not a DIY manual that can be simply adopted by researchers - to implement a system that supports CARE and FAIR research requires development of infrastructure and a governance frameworks. The RRKive principles and website are aimed at those undertaking these substantials tasks. Care Principles - Collective Benefit Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. C1: For inclusive development and innovation Governments and institutions must actively support the use and reuse of data by Indigenous nations and communities by facilitating the establishment of the foundations for Indigenous innovation, value generation, and the promotion of local self-determined development processes. C2: For improved governance and citizen engagement Data enrich the planning, implementation, and evaluation processes that support the service and policy needs of Indigenous communities. Data also enable better engagement between citizens, institutions, and governments to improve decision-making. Ethical use of open data has the capacity to improve transparency and decision-making by providing Indigenous nations and communities with a better understanding of their peoples, territories, and resources. It similarly can provide greater insight into third-party policies and programs affecting Indigenous Peoples. C3: For equitable outcomes Indigenous data are grounded in community values, which extend to society at large. Any value created from Indigenous data should benefit Indigenous communities in an equitable manner and contribute to Indigenous aspirations for wellbeing. Care Principles - Authority to Control Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data. A1 Recognizing rights and interests Indigenous Peoples have rights and interests in both Indigenous Knowledge and Indigenous data. Indigenous Peoples have collective and individual rights to free, prior, and informed consent in the collection and use of such data, including the development of data policies and protocols for collection. A2: Data for governance Indigenous Peoples have the right to data that are relevant to their world views and empower self-determination and effective self-governance. Indigenous data must be made available and accessible to Indigenous nations and communities in order to support Indigenous governance. A3: Governance of data Indigenous Peoples have the right to develop cultural governance protocols for Indigenous data and be active leaders in the stewardship of, and access to, Indigenous data especially in the context of Indigenous Knowledge. Care Principles - Responsibility Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ selfdetermination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples. R1: For positive relationships Indigenous data use is unviable unless linked to relationships built on respect, reciprocity, trust, and mutual understanding, as defined by the Indigenous Peoples to whom those data relate. Those working with Indigenous data are responsible for ensuring that the creation, interpretation, and use of those data uphold, or are respectful of, the dignity of Indigenous nations and communities. R2: For expanding capability and capacity Use of Indigenous data invokes a reciprocal responsibility to","date":"0001-01-01","objectID":"/fundamentals/workspaces-vs-repositories/:0:0","tags":null,"title":"Workspaces vs Repositories","uri":"/fundamentals/workspaces-vs-repositories/"}]