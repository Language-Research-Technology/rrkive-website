[{"categories":null,"content":"RO-Crate is a method for describing and packaging research data from ANY discipline into distributable, reusable Digital Objects with any amount of detailed metadata from simple who/what/where discovery-oriented description to metadata at the file-level or even variable level inside files. RO-Crate is an implementer-focussed guide to best practice and is based on widely-used standards with schema.org annotations in JSON-LD and aims to make it easy to creat good quality metadata description tools which are accessible and practical for use in a wide variety of situations; from an individual researcher working with a folder of data, to large data-intensive computational research environments. RO-Crate is used in the UTS Research Data Portal and the Modern PARADISEC demonstrator for all data objects. RO-Crate is method for describing a dataset as a digital object using a single linked-data metadata document Each resource can have a machine readable description in JSON-LD format A human-readable description and preview can be in an HTML file that lives alongside the metadata Provenance and workflow information can be included - to assist in data and research-process re-use RO-Crate Digital Objects may be packaged for distribution eg via Zip, Bagit and OCFL Objects ","date":"2024-01-01","objectID":"/standards/ro-crate/:0:0","tags":null,"title":"Metadata: Research Object Crate (RO-Crate)","uri":"/standards/ro-crate/"},{"categories":null,"content":"Oxford Common File Layout (OCFL) is a specification for laying out digital collections on file or object storage. It is designed with long-term preservation principles in mind; does not rely on specialised software and avoids the problem of locking data collections into monolithic repositories behind APIs. An OCFL Repository is a system of directories laid out on a filesystem using a prescribed layout. Each repository contains one or more OCFL Objects. An object contains an inventory and a set of versioned content directories. The metadata describing an OCFL object’s inventory and versions is stored as simple JSON files which are both human- and machine-readable, and can be processed with lightweight scripts. The structure inside the content directories is not specified, so any existing collection of files can be deposited into an OCFL repository and later re-exported with its structure preserved. OCFL places no restrictions on the file formats of object contents. ","date":"2024-01-01","objectID":"/standards/ocfl/:0:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"OCFL provides Robustness against file errors and data corruption Efficient versioning and de-duplication Immutable data storage ","date":"2024-01-01","objectID":"/standards/ocfl/:1:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"Links OCFL website OCFL Specification v1.0 ","date":"2024-01-01","objectID":"/standards/ocfl/:2:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"Example Here is a simplified view of an OCFL repository containing two objects, one of which has two versions and one with three: - OCFL repository - Object A - inventory - v1 - inventory - content - v2 - inventory - content - Object B - v1 - inventory - content - v2 - inventory - content - v3 - inventory - content ","date":"2024-01-01","objectID":"/standards/ocfl/:3:0","tags":null,"title":"Standards: Oxford Common File Layout","uri":"/standards/ocfl/"},{"categories":null,"content":"The following flowchart is a (glib) summary of what it takes to adopt the PILARS. See the Implementation section for examples. ","date":"1970-01-01","objectID":"/adoption/:0:0","tags":null,"title":"Aopting PILARS","uri":"/adoption/"},{"categories":null,"content":"Origins – why create the PILARS? The PILARS is an evolution of of the Arkisto website. This was created at the University of Technology Sydney with a small group of partners, and served to bring together a community of like-minded developers implementing CARE and FAIR repositories. The Arkisto work was helpful in that it aided in securing funding and was the starting point for principled development of Archival Repository solutions at LDaCA to give interoperability with PARADISEC. However it had a few issues. The basis of Arkisto was in a set of principles: But we also presented it as a “Platform” And tied implementation to two particular Specifications (which we still use at LDaCA) Critically, the site did not have clear ownership or governance so there was no mechanism for resolving disagreements or making major changes to the site So, we started the PILARS work to specify some protocols for implementing CARE and FAIR compliant Archival Repositories. ","date":"1970-01-01","objectID":"/background/:1:0","tags":null,"title":"Background","uri":"/background/"},{"categories":null,"content":"The need for PILARS As with Arkisto, the driver for creating the PILARS was gap in the infrastructure available to communities trying to implement Data Commons or establish archival repositories; in many cases there were no obvious existing places to host data, and many of the technical solutions available are not fit for purpose for presenting collections of data in a usable way, with low cost and low risk. The following flow-chart illustrates the issue. In choosing a repository service for data, institutional data repositories (where they exist) are typically geared to deal with discrete data packages, not 1000s of images or recordings or samples that researchers might want to access individually, or in filtered sets and are usually geared for open access only. The tools we are developing at LDaCA, with our partners are designed to offer solutions to any data-archive project where there are no existing services, and no institutional resources on hand (such as a special collections unit in a library). Read more about adopting RRKive software solutions to make PILARS compliant Archival Repositories ","date":"1970-01-01","objectID":"/background/:2:0","tags":null,"title":"Background","uri":"/background/"},{"categories":null,"content":"Contact Us ","date":"1970-01-01","objectID":"/contact/:0:0","tags":null,"title":"Contact Us","uri":"/contact/"},{"categories":null,"content":"This page has summaries and links of known implementations of the PILARS. There are three software toolkits that implement the Protocols at least in part. All of these implementations use RO-Crate metadata for the PILARS Protocol 2. The Oni platform, which uses the Oxford Common File Layout as a storage layer Nyingarn Workspace which uses NOCFL - a storage library which is mostly PILARS compliant but which does not (as far as we know) comply with Protocol 1.3 – as IDs do not translate directly to storage locations, unlike in PARADISEC. PARADISEC Archive Code, which uses PROXYIST, wrapper for OCFL and a simpler file-like layout using object storage (S3). PARADISEC uses the Nabu system to manage its archive. This comprises an SQL database to store metadata and a storage layer to store the archive items. The storage layer is abstracted by Proxyist, which provides a simple REST API in an object store-like fashion. Proxyist supports multiple backend storage mechanisms via a plugin mechanism. It can support combinations of local disk or S3 storage and supports plain directory storage or OCFL mechanisms. In the case of PARADISEC, Proxyist is being used to interface to an AWS S3 bucket with a simple file structure layout. The root of the bucket contains one directory for each collection in the repository. Each collection directory contains one directory for each item in the collection. The item directories contain all the essences of the item. This typically comprises archival and presentation versions of media like images, audio and video, and non-media items like PDF, ELAN, XLSX, etc. Alongside these files, a metadata file describing the item is stored. This is currently a custom XML file but will soon be replaced by an ROCrate JSON file. This diagram shows the relationship between various implementations, the software toolkits they use and the specifications on which they are based. ","date":"1970-01-01","objectID":"/implementations/:0:0","tags":null,"title":"Implemetations of PILARS","uri":"/implementations/"},{"categories":null,"content":"Image summary for content/images ","date":"0001-01-01","objectID":"/images/_preview_/:0:0","tags":null,"title":"","uri":"/images/_preview_/"},{"categories":null,"content":"content/images/ardc-repo-chooser-2.svg ","date":"0001-01-01","objectID":"/images/_preview_/:1:0","tags":null,"title":"","uri":"/images/_preview_/"},{"categories":null,"content":"content/images/rrkive-implementations.svg ","date":"0001-01-01","objectID":"/images/_preview_/:2:0","tags":null,"title":"","uri":"/images/_preview_/"},{"categories":null,"content":"content/images/steps-to-archival-repositories.svg ","date":"0001-01-01","objectID":"/images/_preview_/:3:0","tags":null,"title":"","uri":"/images/_preview_/"},{"categories":null,"content":"(This page is a work in progress, first version is a quick update from the Arkisto website: will be updated with fresh examples in Q2 2024) There are multiple use cases for RRKive which we will document in the abstract, in addition to the specific case studies we’re already working on. Due to its standards-based and extensible nature, RRKive can realise the goal of making data FAIR (Findable, Accessible, Interoperable, Re-usable). The (mythical) minimal RRKive platform The simplest possible RRKive platform deployment would be a repository with some objects in it. No portal, no data ingest or preservation service, eg: That might not seem very useful in itself but a standards-based repository is a great candidate for data preservation - it puts the I in FAIR (Interoperable) data. It can also provide a basis for re-activation via services that can Reuse the data by making Findable and Accessible. Because of RRKive’s use of Standards, the repository is the core to which services can be added. Adding a web portal To make the data Findable - the F in FAIR Data a portal may be added - this requires some configuration but significantly less than building a service from scratch. For example the Oni toolkit is used by the Language Data Commons of Australia for its portals, the main one being https://data.ldaca.edu.au/search The same portal software has been used in other contexts and is being adapted at the University of Sydney for a variety of data collections. Web portals depend on the repository content being indexed - web pages are generated mostly from calls to the index, and thus can be highly performant. There is more on indexing below. Data ingest pathways But how does data get into an OCFL repository? There are several patterns in production, in development and planned. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:0:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"A snapshot repository Exporting a repository to RO-Crate can be a matter of writing a script to interrogate a database, convert static files, or otherwise traverse an existing dataset. This pattern was used by the 2020 snapshot of ExpertNation - where we were given an XML file exported from the Heurist Content Management System and used a script to convert that data to the RO-Crate format. This RO-Crate can in turn be deposited in a repository - in this case the UTS data repository - and served via a portal, preserving the data while at the same time making it Accessible. This pattern has also been used extensively in the Language Data Commons of Australia where individual collections of data are converted using a series of scripts - instead of creating an entire site from one collection as in Expert Nation, each collection is loaded into the site. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:1:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Field data capture Data from sensors in the field is often streamed directly to some kind of database with or without portal services and interfaces. There are multiple possible RRKive deployment patterns in this situation. Where practical, the UTS eResearch team aimed to take an approach that first keeps copies of any raw data files and preserves those. The team then built databases and discovery portals from the raw data, although this is not always possible. This diagram shows an approximation of one current scenario which was implemented at UTS where raw files were NOT available, but readings could be extracted from a Database: ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:2:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Analytical and housekeeping tools So far on this page we have covered simplified views of RRKive deployment patterns with the repository at the centre, adding discovery portals and giving examples of data-acquisition pipelines (just scratching the surface of the possibilities). These things in themselves have value: making sure data is well described and as future proof as possible are vitally important but what can we DO with data? ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"OCFL + RO-Crate tools Having data in a standard-stack, with OCFL used to lay-out research data objects and RO-Crate to describe them, means that it is possible to write simple programs that can interrogate a repository. That is, you don’t have to spend time understanding the organisation of each dataset. The same idea underpins RRKive’s web-portal tools: standardization reduces the cost of building. Validators and preservation tools: there are not many of these around yet, but members of the RRKive community as well as the broader OCFL and RO-Crate communities are working on these; expect to see preservation tools that work over OCFL repositories in particular. Brute-force scripts: for moderate-sized repositories, it is possible to write a scripts to examine every object in a repository and to run analytics. For instance, it would be possible to visualise a number of years’ worth of sensor readings from a small number of sensors or to look for the geographical distribution of events in historical datasets. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:1","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Adding databases and other indexes For larger-scale use, visiting every object in a repository can be inefficient. In these cases, using an index means that an analysis script can request all the data in a particular time-window or of a certain type - or any other query that the index supports. While the index engines used in our current portals are based on full-text search and metadata, we expect others to be built as needed by disciplines using, for example, SQL databases or triple stores. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:2","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Analysis tool integration Above, we have looked at how people or machines can access an RRKive platform deployment by querying the repository, either directly or via an index. However, there is a much larger opportunity in being able to integrate RRKive deployments with other tools in the research landscape. To take one example, text analysis is in great demand across a very wide range of disciplines. This hypothetical scenario shows the potential for a researcher to use a web portal to locate datasets which contain text and then send the whole results set to a an analysis platform, in this case an interactive Jupyter notebook. RRKive already allows re-use of visualisation tools and viewers that can be embedded directly in a portal. We are planning a “widget store” that will enable researchers to choose and deploy a number of add-ons to the basic portal. Institutional and discipline repositories One of the major use case deployment patterns for RRKive is to underpin an institutional data repository / archive function, see the UTS data repository for an established example. In this use case, data is ingested into the repository via a research data management system which talks to the OCFL repository, not the portal. There is a clear separation of concerns: the portal’s job is to provide controlled access and search services via an index, the OCFL repository keeps version controlled data on disc, and the Research Data Management System handles deposit of data. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:3:3","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Manual archiving At UTS the Research Data Management system in use is RedBox - an open source platform for managing data across the research process from research data management planning (via Research Data Management Plans (RDMPS)) to archiving and re-use of data. ReDBox has services for provisioning and/or tracking Research Workspaces, which are sites where research data collection and management. All of the data acquisition scenarios described above would qualify as Research Workspaces, as do file-shares on institutional storage or share-sync services such as CloudStor, as well as survey platforms, project management and version control systems such as Gitlab and Github. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:4:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Publishing data The UTS institutional data repository actually has two parts: an internal behind-the-firewall archive, with access control - to ensure that only authorized people can access data - and an external data portal for publicly accessible data. This architecture reduces the risk of data breaches by not allowing access through the firewall to sensitive or confidential data until secure tools are available to allow extra-institutional access. Researchers can select a subset of an archived dataset to be published, or publish an entire dataset. A “bot” “notices” that a new public dataset is available and copies it to the public repository, where it will be indexed and made available through the data portal. NOTE: There is no monolithic “Repository Application” that mediates all interactions with the file-base OCFL store but a set of services which operate independently. This does mean that processes must be in place to ensure that there is not file-contention, with two bits of software trying to update an object at the same time. ","date":"0001-01-01","objectID":"/fundamentals/implementation-patterns/:5:0","tags":null,"title":"Use Cases, Software Architecture and Deployment Patterns","uri":"/fundamentals/implementation-patterns/"},{"categories":null,"content":"Worspaces vs Repositories ::: notes There is a fundamental distinction between repositories and workspaces and the role they play in a FAIR-compliant research workflow ::: Examples of Workspaces Generic workspace examples include: Survey tools (Survey Monkey, ReDCAP) Electronic Lab Notebooks Code environments (Jupyter notebooks, Binderhub, Github*, Matlab, R Studio) Research \u0026 Analytical Databases** “R: Drive” or similar storage Cloud storage such as Dropbox, Google Drive or OneDrive *Yes, git uses “repositories” but these do not function as archives – don’t assume Microsoft, the current owner of github, will preserve research or other code **No a database is not necessarily a repository (more on that below) Features of a repository ⬇️ Governance / Policy ⬇️ ⬇️ Technology ⬇️ Purpose / mission Data retention policies Planning: ongoing stewardship \u0026 contingencies persistence of IDs data exit software obsolescence Deposit and use and redistribution licensing/permissions framework Tested data-exit pathway* Interoperable metadata framework APIs Implementation of ID resolution \u0026 updating Discovery services / catalogues / indicies Collection / archival structures Full-text \u0026 semantic indexing Facets for discovery based on explicit and implicit metadata Sustainable technology neutral access-control mechanisms *Get this right and the other things follow ⬆️ Repo as Institution ⬆️ ⬆️ Repo Implementation ⬆️ ::: notes A repository is as much an institution as it is a software implementation. ::: CARE / FAIR ⬇️ Governance / Policy ⬇️ ⬇️ Technology ⬇️ CARE Principles FAIR Principles ⬆️ Repo as Institution ⬆️ ⬆️ Repo Implementation ⬆️ Differences between Repositories and Archives (and why we say Archival Repository) Repository and Archive are closely related terms used by different communities – they both have a core meaning about ‘keeping’ something for an appropriate time span. Some non-archivists might quip that archives are where things go never to be seen again while to non-archivists Repositories, or digital libraries may lack the rigour of proper archival practice and use unfamiliar organizing principles. Repositories, in the Higher Education sector are probably best known for their role in the Open Access publications movement – while these are typically called “Institutional Repositories”, it is helpful to think of them as “Institutional Publications Repositories”. Many of these do contain research data, but they typically do not have access controls for non-open data, have size limits for deposits and do not have built in domain-extensibility. In the RRKive project we aim to set aside these interesting discussions in the interest of finding common ground so we use the term Archival Repository. ","date":"0001-01-01","objectID":"/fundamentals/workspaces-vs-repositories/:0:0","tags":null,"title":"Workspaces vs Repositories","uri":"/fundamentals/workspaces-vs-repositories/"}]